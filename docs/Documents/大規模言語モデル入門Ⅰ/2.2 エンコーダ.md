---
title: 2.2 エンコーダ
tags:
  - 大規模言語モデル入門Ⅰ
---

各ブロックでどのような処理が行われているか解説をしていく前に、  
入力から出力まで、入力した内容がどのように変化していくかざっくり示す。  
下記図のように入力された単語がどのように変化していくか、  
計算式と次元数に着目して記載した。
  

![alt text](<../../images/2.2 エンコーダ/2.2 エンコーダ_J0133291.png>)

| **記号**  | **説明**                                 |
|-----------|------------------------------------------|
| **Z'**    | 残差結合&正規化後のZ                      |
| **Z**     | Feed Forward後のO'                       |
| **Df**    | 隠れ層の次元数(論文だと4D)                |
| **O'**    | 残差結合&正規化後のO                      |
| **O**     | Multi-Head Attentionの処理後              |
| **Q, K, V**| Query, Key, Value                       |
| **X**     | Eに位置符号を追加したもの                 |
| **E**     | Embedding後のtoken                       |
| **D**     | 入力Embeddingの次元数D（入力単語をD次元のベクトルに変換）|
| **n**     | 語彙数（token数）                         |

<div class="chat-container">
  <!-- ずんだもんの右寄せ質問 -->
  <div class="chat right">
    <div class="chat-message zundamon-message">
      なんか色々計算式が出てきて分かりにくいのだ...<br>
      結局何をやっているのだ？
    </div>
    <img src="../../images/2.1 概要/2.1 概要_J0133291-2.png" alt="ずんだもん" class="avatar">
  </div>

  <!-- 春日部つむぎの左寄せ回答 -->
  <div class="chat left">
    <img src="../../images/2.2 エンコーダ/2.2 エンコーダ_J0133291-15.png" alt="春日部つむぎ" class="avatar">
    <div class="chat-message tsumugi-message">
      あ～、確かにちょっとややこしいよね💦<br>
      簡単に言うと、<br>
      Encoderは日本語の文を「こういう意味だな」って理解するために使うの。<br>
      例えば「あめが降る」って入力すると、<br>
      最初に<strong>Embedding</strong>で単語を数値に変換するんだよ。<br>
      でもこの時点では「あめ」が「飴」か「雨」かまだ分からないんだ🌧️🍬
    </div>
  </div>

  <!-- 追加説明 -->
  <div class="chat left">
    <img src="../../images/2.1 概要/2.1 概要_J0133291-3.png" alt="春日部つむぎ" class="avatar">
    <div class="chat-message tsumugi-message">
      Encoderの計算で、文脈を理解して「あめ」と「降る」が関係してるから、<br>
      「雨」っていう意味に変わるの！✨<br>
      最初のEmbeddingでは文脈は分からないけど、<br>
      処理後は文全体の意味を考慮したベクトルになるんだ😊
    </div>
  </div>
    <div class="chat left">
    <img src="../../images/2.2 エンコーダ/2.2 エンコーダ_J0133291-1.png" alt="春日部つむぎ" class="avatar">
    <div class="chat-message tsumugi-message">
      次は、順番に具体的な処理を解説していくね！
    </div>
  </div>
</div>

!!! warning
    本と一部ベクトルの表現が異なり、論文に寄せた表記になります。   
    なのでFeed Forwardの計算式が普段見かけるものと違うように見えるかもしれませんが、  
    行っていることは同様です。     
    以下の項での説明も同様に一部本と異なる表現を行っています。  (本の内容だと次元が分かりにくいため)
    また本の内容だと所々通例として使用される変数の文字が入っていたりしたので、  
    ここでは前後の繋がりを加味した変数の文字に書き換えています。

!!! warning
    上の図のMulti-Head Attentionの中身の次元数は  理解のしやすさを優先し、self-attentionにしています。  
    (過程が異なるだけで、出力結果の次元数は変わらないので、特に気にしなくても大丈夫です)

<br>

## 2.2.1 入力トークン埋め込み
---
<div class="chat-container">
  <div class="chat left">
    <img src="../../images/2.1 概要/2.1 概要_J0133291-3.png" alt="春日部つむぎ" class="avatar">
    <div class="chat-message tsumugi-message">
      まずは、前の1.3の章でも出てきたけど、<br>
      最初に言葉をベクトルに変換するよ！  <br>
      これが<strong>Embedding</strong>ってやつだね💡
    </div>
  </div>
</div>

<figure markdown="span">
  ![Image title](<../../images/2.2 エンコーダ/2.2 エンコーダ_J0133291-2.png>){ width="400" }
  <figcaption>Input Embedding</figcaption>
</figure>

!!! abstract
    入力した語彙をn個のトークンに分割して、 ベクトル**E**に変換。

<br>

## 2.2.2 位置符号
---

<div class="chat-container">
  <div class="chat left">
    <img src="../../images/2.1 概要/2.1 概要_J0133291-3.png" alt="春日部つむぎ" class="avatar">
    <div class="chat-message tsumugi-message">
      次にね、<strong>位置符号（Positional Encoding）</strong>っていうのも大事なの！<br>
      Transformerは単語を並列で処理するから、<br>
      どの単語がどの位置にあるかを自動では分からないんだよね😅<br>
      そこで、この位置符号を使って単語の順番を教えてあげるの<br>
    </div>
  </div>
</div>

<figure markdown="span">
  ![Image title](<../../images/2.2 エンコーダ/2.2 エンコーダ_J0133291-3.png>){ width="400" }
  <figcaption>positional encodingを付与</figcaption>
</figure>

<div class="chat-container">
  <!-- 春日部つむぎの説明 -->
  <div class="chat left">
    <img src="../../images/2.2 エンコーダ/2.2 エンコーダ_J0133291-1.png" alt="春日部つむぎ" class="avatar">
    <div class="chat-message tsumugi-message">
      具体的には、位置符号は<strong>sin</strong>と<strong>cos</strong>を使って計算されるんだ💡<br>
      トークンの位置 <em>i</em> に対して、以下の数式で位置情報が作られるんだよ👇<br>
      $$ p_{i,2k+1} = \sin\left(\frac{i}{10000^{2k/D}}\right) $$
      $$ p_{i,2k+2} = \cos\left(\frac{i}{10000^{2k/D}}\right) $$
      この <strong>k</strong> は、位置符号の次元を分割して管理するためのインデックスなの。<br>
      次元の半分は <strong>sin</strong> で、もう半分は <strong>cos</strong> で計算してるんだ😊
    </div>
  </div>

  <!-- ずんだもんの質問 -->
  <div class="chat right">
    <div class="chat-message zundamon-message">
      結局、<strong>p</strong> には、何が入っているのだ？<br>
      数式が別れていて分かりにくいのだ!!
    </div>
    <img src="../../images/2.1 概要/2.1 概要_J0133291-2.png" alt="ずんだもん" class="avatar">
  </div>

  <!-- 春日部つむぎの説明 -->
  <div class="chat left">
    <img src="../../images/2.1 概要/2.1 概要_J0133291-11.png" alt="春日部つむぎ" class="avatar">
    <div class="chat-message tsumugi-message">
      あ、ごめんね！<br>
      じゃあ、数式をまとめるとこうなるよ<br>

      $$ 
      p_i = 
      \begin{bmatrix}
      \sin(i) \\
      \cos(i) \\
      \sin\left(\frac{i}{10000^{2/D}}\right) \\
      \cos\left(\frac{i}{10000^{2/D}}\right) \\
      \vdots \\
      \sin\left(\frac{i}{10000^{(D-2)/D}}\right) \\
      \cos\left(\frac{i}{10000^{(D-2)/D}}\right)
      \end{bmatrix}
      $$
      
      分かったかな？説明に戻るね!
    </div>
  </div>

  <!-- 春日部つむぎの追加説明 -->
  <div class="chat left">
    <img src="../../images/2.1 概要/2.1 概要_J0133291-3.png" alt="春日部つむぎ" class="avatar">
    <div class="chat-message tsumugi-message">
      最後に、この式
      $$ x_i = \sqrt{D} \cdot e_{i} + p_i $$
      で、位置符号がトークンの埋め込みに追加されるんだ！<br>
      <strong>\(\sqrt{D}\)</strong> が使われている理由は、トークンのベクトルが大きくなりすぎないようにスケールを調整してるの👍<br>
      これで、どの位置にあるかしっかり考慮して処理できるんだよ😊！
    </div>
  </div>

　<!-- ずんだもんの質問 -->
  <div class="chat right">
    <div class="chat-message zundamon-message">
      なんで<strong>sin</strong>とか<strong>cos</strong>なんか使ってるのだ!?<br>
      もっと簡単な方法とかないのだ!?<br>
    </div>
    <img src="../../images/2.2 エンコーダ/2.2 エンコーダ_J0133291-4.png" alt="ずんだもん" class="avatar">
  </div>

  <!-- 春日部つむぎの追加説明 -->
  <div class="chat left">
    <img src="../../images/2.2 エンコーダ/2.2 エンコーダ_J0133291-5.png" alt="春日部つむぎ" class="avatar">
    <div class="chat-message tsumugi-message">
      sinとcosを使うと、周期的な性質を持っていて、<br>
      位置を表すのにピッタリで、計算も効率的だからなんだよ～✨<br>
      <a href="https://www.nomuyu.com/positional-encoding/#st-toc-h-2" target="_blank">
      ここ</a>に詳しく書いてあるから読んでみてね！
    </div>
  </div>
</div>

!!! abstract
    tokenに対してinput Embeddingをした後のベクトル「**E**」に対して  
    位置符号を付与してベクトル「**X**」にする

<br>

## 2.2.3 自己注意機構
---

<figure markdown="span">
  ![Image title](<../../images/2.2 エンコーダ/2.2 エンコーダ_J0133291-12.png>){ width="600" }
  <figcaption>self-Attention</figcaption>
</figure>

<div class="chat-container">
  <div class="chat left">
    <img src="../../images/2.2 エンコーダ/2.2 エンコーダ_J0133291-16.png" alt="春日部つむぎ" class="avatar">
    <div class="chat-message tsumugi-message">
      ついに、Attention機構の一つ、<strong>self-attention</strong>について説明するね！💡<br>
      self-attentionを計算する方法<strong>「Scaled Dot-Product Attention」</strong>について解説していくよ～✨
    </div>
  </div>
</div>

<figure markdown="span">
  ![Image title](<../../images/2.2 エンコーダ/2.2 エンコーダ_J0133291-6.png>){ width="400" }
  <figcaption>Scalied Dot-Product Attentionの模式図</figcaption>
</figure>

| **要素**     | **説明**                                                    |
|--------------|-------------------------------------------------------------|
| **Query (Q)**| 自分が「知りたいこと」や「注目したい情報」を表現するベクトル    |
| **Key (K)**  | 文中の他の単語の「特徴や情報」を表現するベクトル               |
| **Value (V)**| 実際に「取り出したい情報」を表すベクトル                       |
| **d<sub>k</sub>** | QueryとKeyの次元数。内積計算で値が大きくなりすぎるのを防ぐために使用|

複雑な式に見えるが、結局やっていることは下記の式を計算しているだけである。
<div class="math">
  $$ \text{softmax}\left(\frac{QK^T}{\sqrt{d_k}}\right)V $$
</div>

<br>

!!! question
    1. Q,K,Vはどこから導かれているのか？
    2. この計算を通して何をしているのか？

### 1. Q,K,Vはどこから導かれているのか？

<div class="chat-container">
  <!-- 春日部つむぎの解説 -->
  <div class="chat left">
    <img src="../../images/2.1 概要/2.1 概要_J0133291.png" alt="春日部つむぎ" class="avatar">
    <div class="chat-message tsumugi-message">
      Q、K、Vはそれぞれ、<br>
      入力ベクトル<strong>X</strong>に重み行列<strong>W</strong>を掛けて導き出されるんだ。<br>
      式にするとこうなるよ👇<br>
      $$ Q = XW_Q $$
      $$ K = XW_K $$
      $$ V = XW_V $$
    　<strong>W<sub>Q</sub></strong>、<strong>W<sub>K</sub></strong>、<strong>W<sub>V</sub></strong>はそれぞれ違う値の重み行列だよ！✨
    </div>
  </div>
</div>  

<figure markdown="span">
  ![Image title](<../../images/2.2 エンコーダ/2.2 エンコーダ_J0133291-7.png>){ width="600" }
  <figcaption>Q,K,Vの計算のイメージ図</figcaption>
</figure>

<div class="chat-container">
  <!-- 春日部つむぎの追加説明 -->
  <div class="chat left">
    <img src="../../images/2.1 概要/2.1 概要_J0133291-3.png" alt="春日部つむぎ" class="avatar">
    <div class="chat-message tsumugi-message">
      じゃあ、なんでQ、K、Vに分けるのか？って思うよね？<br>
      <strong>Q（Query）</strong>は「どの単語に注目してるか」、<br>
      <strong>K（Key）</strong>は「他の単語がどういう情報を持ってるか」、<br>
      <strong>V（Value）</strong>は「実際に取り出す情報」なんだ😊<br>
      この3つに分けることで、<br>
      文中の単語同士がどれだけ関係してるかを効率的に計算できるんだよ！<br>
      <strong>Q</strong>と<strong>K</strong>を使って関連度を計算して、<br>
      それを基に<strong>V</strong>から本当に必要な情報を引き出すって流れだよ～✨
    </div>
  </div>

  <!-- ずんだもんの右寄せ質問 -->
  <div class="chat right">
    <div class="chat-message zundamon-message">
      \( W_Q \), \( W_K \), \( W_V \) はどこから導き出されるのだ?<br>
    </div>
    <img src="../../images/2.1 概要/2.1 概要_J0133291-8.png" alt="ずんだもん" class="avatar">
  </div>
  <!-- 春日部つむぎの解説 -->
  <div class="chat left">
    <img src="../../images/2.1 概要/2.1 概要_J0133291.png" alt="春日部つむぎ" class="avatar">
    <div class="chat-message tsumugi-message">
      あ、これらの重み行列はね、<br>
      モデルが学習する過程でどんどん調整されるんだよ！<br>
      最初はランダムだけど、学習データを使って、徐々にちゃんとした重みに調整されていくんだ✨
    </div>
  </div>
</div>

<br>

### 2. この計算を通して何をしているのか
<div class="chat-container">
  <!-- 春日部つむぎの解説 -->
  <div class="chat left">
    <img src="../../images/2.2 エンコーダ/2.2 エンコーダ_J0133291-1.png" alt="春日部つむぎ" class="avatar">
    <div class="chat-message tsumugi-message">
      じゃあ次は、この式が何してるか解説するね！<br>
      $$ \text{softmax}\left(\frac{QK^T}{\sqrt{d_k}}\right)V $$<br>
      まず、<strong>Q</strong>と<strong>K</strong>の内積を取ることで、<br>
      各単語が他の単語とどれくらい関連してるかを計算するんだよ～✨<br>
      内積の値が大きいほど、その単語同士が関連してるってことだね！<br>
      これはさっき説明した、<strong>Q</strong>と<strong>K</strong>を使って似てる単語を見つけるのと同じだよ😊<br>
      でも次元数 \( d_k \) が大きくなると内積が大きくなりすぎるから、平方根で割って調整してるんだよ！
    </div>
  </div>

  <!-- 春日部つむぎの説明の続き -->
  <div class="chat left">
    <img src="../../images/2.1 概要/2.1 概要_J0133291.png" alt="春日部つむぎ" class="avatar">
    <div class="chat-message tsumugi-message">
      それから、softmaxを使って関連度を0から1の間で正規化して、<br>
      どの単語にどれだけ注目するかが決まるの！<br>
      最後に、その重みを<strong>V（Value）</strong>に掛けて、<br>
      実際に必要な情報を引き出すって感じだね～✨
    </div>
  </div>
</div>

!!! abstract
    1. Q,K,Vはどこから導かれているのか？<br>　
    XにWを掛けてQ,K,Vを導く　　
    2. この計算を通して何をしているのか？<br>  　
    「どの単語に注目するべきか」を決めて、その単語から必要な情報を取り出してる

<br>

## 2.2.4 マルチヘッド注意機構

<div class="chat-container">
  <!-- 春日部つむぎの解説 -->
  <div class="chat left">
    <img src="../../images/2.1 概要/2.1 概要_J0133291.png" alt="春日部つむぎ" class="avatar">
    <div class="chat-message tsumugi-message">
      次は<strong>Multi-Head Attention</strong>について詳しく説明するね！💡<br>
      Self-Attentionでは、文中の単語同士の関連度を計算して、<br>
      どの単語に注目するかを決めるんだけど、<br>
      Multi-Head Attentionはそれを<strong>複数の視点</strong>で同時に行うんだ。<br>
      複数の視点を持つヘッドがあって、<br>
      それぞれちょっと違う視点から計算しているよ✨
    </div>
  </div>

  <!-- ずんだもんの右寄せ質問 -->
  <div class="chat right">
    <div class="chat-message zundamon-message">
      ヘッド？それって何をするのだ？
    </div>
    <img src="../../images/2.1 概要/2.1 概要_J0133291-8.png" alt="ずんだもん" class="avatar">
  </div>

  <!-- 春日部つむぎの式の説明 -->
  <div class="chat left">
    <img src="../../images/2.2 エンコーダ/2.2 エンコーダ_J0133291-15.png" alt="春日部つむぎ" class="avatar">
    <div class="chat-message tsumugi-message">
      あ、そうだよね、ヘッドって分かりにくいよね💦<br>
      ヘッドは、1つの視点で「どの単語に注目するか」を計算する部分だよ！<br>
      各ヘッドはそれぞれの役割で<strong>Scaled Dot-Product Attention</strong>をやってて、<br>
      例えば「主語と動詞の関係」とか「目的語との関係」とか、別々の視点から情報を見てるんだ💡
    </div>
  </div>

  <!-- ずんだもんの右寄せ質問 -->
  <div class="chat right">
    <div class="chat-message zundamon-message">
      つまり、いくつかの違う視点で同時に計算するってことなのだ？
    </div>
    <img src="../../images/2.1 概要/2.1 概要_J0133291-9.png" alt="ずんだもん" class="avatar">
  </div>

  <!-- 春日部つむぎの解説の続き -->
  <div class="chat left">
    <img src="../../images/2.1 概要/2.1 概要_J0133291-5.png" alt="春日部つむぎ" class="avatar">
    <div class="chat-message tsumugi-message">
      そうそう！まさにそれ！😊<br>
      複数のヘッドがそれぞれ違う視点で文の意味を捉えて、<br>
      最後にそれを全部まとめて1つの情報にするんだ！✨<br>
      これがMulti-Head Attentionのすごいところなんだよ～。
    </div>
  </div>
</div>

<figure markdown="span">
  ![Image title](<../../images/2.2 エンコーダ/2.2 エンコーダ_J0133291-8.png>){ width="600" }
  <figcaption>Multi-Head AttentionでのQ',K',V'計算のイメージ図</figcaption>
</figure>

<div class="chat-container">
  <!-- 春日部つむぎのMulti-Head Attentionの式の説明 -->
  <div class="chat left">
    <img src="../../images/2.1 概要/2.1 概要_J0133291.png" alt="春日部つむぎ" class="avatar">
    <div class="chat-message tsumugi-message">
      さっきのイメージ図をMulti-Head Attentionの場合に変更したよ💡<br>
      <strong>Q,K,V</strong>に、
      <strong>W<sub>i</sub><sup>Q</sup>,W<sub>i</sub><sup>K</sup>,W<sub>i</sub><sup>V</sup></strong>
      を掛けてできる<strong>Q',K',V'</strong>を使用するんだ。<br>
      <strong>W<sub>i</sub><sup>Q</sup>,W<sub>i</sub><sup>K</sup>,W<sub>i</sub><sup>V</sup></strong>は
      モデルの次元Dをヘッド数hで割った次元をもつ重み行列だよ
    </div>
  </div>

  <div class="chat left">
    <img src="../../images/2.1 概要/2.1 概要_J0133291.png" alt="春日部つむぎ" class="avatar">
    <div class="chat-message tsumugi-message">
      そして、各ヘッドで計算された出力head<sub>1</sub>,..,head<sub>h</sub>を全部結合して、<br>
      最終的に使うのがこの式だよ👇<br>
      $$ \text{MultiHead}(Q, K, V) = \text{Concat}(\text{head}_1, \text{head}_2, ..., \text{head}_h)W_O $$
      結合された出力に対して、<strong>W<sub>O</sub></strong>という重み行列を使って、<br>
      最終的な出力に変換するんだ！💡
    </div>
  </div>

  <!-- ずんだもんの右寄せ感想 -->
  <div class="chat right">
    <div class="chat-message zundamon-message">
      これでいろんな視点で文の意味を見ることができるのだな！
    </div>
    <img src="../../images/2.1 概要/2.1 概要_J0133291-9.png" alt="ずんだもん" class="avatar">
  </div>
    <!-- ずんだもんの右寄せ質問 -->
  <div class="chat right">
    <div class="chat-message zundamon-message">
      ところで、head<sub>1</sub>？ってなんのことなのだ？<br>
      各ヘッドで計算された出力っって言われてもよく分からないのだ。
    </div>
    <img src="../../images/2.1 概要/2.1 概要_J0133291-4.png" alt="ずんだもん" class="avatar">
  </div>

  <!-- 春日部つむぎの追加説明 -->
  <div class="chat left">
    <img src="../../images/2.1 概要/2.1 概要_J0133291-3.png" alt="春日部つむぎ" class="avatar">
    <div class="chat-message tsumugi-message">
        head<sub>1</sub>って言ってるのは、Self-Attentionででてきたこの式になるよ👇<br>
        $$ \text{softmax}\left(\frac{QK^T}{\sqrt{d_k}}\right)V $$
        各ヘッドのこの結果をまとめてるから、最終的にはSelf-Attentionの出力と次元数は同じになるの！
    </div>
  </div>
</div>

!!! abstract
    複数の視点から文中の単語同士の関連性を同時に計算し、<br>
    それらの結果を統合することで、文脈の異なる側面をより深く理解できる仕組み<br>
    出力結果**O**の次元数はself-attentionとMulti-Head attentionで同じ

<br>

## 2.2.5 フィードフォワード層

<figure markdown="span">
  ![Image title](<../../images/2.2 エンコーダ/2.2 エンコーダ_J0133291-11.png>){ width="600" }
  <figcaption>feed forward</figcaption>
</figure>

<div class="chat-container">
  <!-- 春日部つむぎの解説 -->
  <div class="chat left">
    <img src="../../images/2.1 概要/2.1 概要_J0133291.png" alt="春日部つむぎ" class="avatar">
    <div class="chat-message tsumugi-message">
      じゃあ次は、<strong>フィードフォワード層</strong>について説明するね！💡<br>
      フィードフォワード層は、Self-Attentionの処理が終わった後に、<br>
      単語ごとに行われる処理なんだ。<br>
      これによって、Attentionで得られた情報をさらに深く学習できるようにするよ！
    </div>
  </div>

  <!-- ずんだもんの右寄せ質問 -->
  <div class="chat right">
    <div class="chat-message zundamon-message">
      Attentionの後にさらに何かするのだ!?<br>
      Attentionだけで十分な気がするのだ…？
    </div>
    <img src="../../images/2.1 概要/2.1 概要_J0133291-2.png" alt="ずんだもん" class="avatar">
  </div>

  <!-- 春日部つむぎの続き -->
  <div class="chat left">
    <img src="../../images/2.2 エンコーダ/2.2 エンコーダ_J0133291-15.png" alt="春日部つむぎ" class="avatar">
    <div class="chat-message tsumugi-message">
      そうだよね～！でもね、Attentionは基本的に線形な計算だから、<br>
      文の複雑な関係をもっとよく捉えるために、<strong>非線形性</strong>が必要なんだ💡<br>
      そこでフィードフォワード層が使われて、<br>
      入力された単語ベクトルを一旦大きく広げて、<br>
      そこで複雑なパターンを学習できるようにしてるんだ！
    </div>
  </div>

  <!-- ずんだもんの右寄せ質問 -->
  <div class="chat right">
    <div class="chat-message zundamon-message">
      えっ？どうやって広げるのだ？
    </div>
    <img src="../../images/2.1 概要/2.1 概要_J0133291-8.png" alt="ずんだもん" class="avatar">
  </div>

  <!-- 春日部つむぎの続き -->
  <div class="chat left">
    <img src="../../images/2.1 概要/2.1 概要_J0133291-3.png" alt="春日部つむぎ" class="avatar">
    <div class="chat-message tsumugi-message">
      フィードフォワード層は<strong>2層の全結合ネットワーク</strong>でできてるんだ！<br>
      これを通して、複雑な情報を学習できるんだよ！
    </div>
  </div>

  <!-- ずんだもんの右寄せ質問 -->
  <div class="chat right">
    <div class="chat-message zundamon-message">
      ...
    </div>
    <img src="../../images/2.2 エンコーダ/2.2 エンコーダ_J0133291-9.png" alt="ずんだもん" class="avatar">
  </div>
</div>

<div class="chat-container">
  <!-- 春日部つむぎの数式の説明 -->
    <div class="chat left">
    <img src="../../images/2.2 エンコーダ/2.2 エンコーダ_J0133291-17.png" alt="春日部つむぎ" class="avatar">
    <div class="chat-message tsumugi-message">
      <img src="../../images/2.2 エンコーダ/2.2 エンコーダ_J0133291-10.png" alt="フィードフォワード層のイメージ図">
    </div>
  </div>

  <div class="chat left">
    <img src="../../images/2.1 概要/2.1 概要_J0133291.png" alt="春日部つむぎ" class="avatar">
    <div class="chat-message tsumugi-message">
      よく見るこれが二層の全結合ネットワークね<br>
      数式にするとこんな感じ👇<br>
      $$ Z = \ f(O W_1 + b_1)W_2 + b_2 $$
      まず、最初に<strong>W<sub>1</sub></strong>って重み行列を掛けて次元を広げて、<br>
      それから<strong>GELU</strong>っていう活性化関数fを使って非線形な変換をしてるの。<br>
      最後に<strong>W<sub>2</sub></strong>って重み行列を掛けて、元の次元に戻してるんだ😊
    </div>
  </div>
</div>

!!! abstract
    非線形性を獲得するために二層のNNを通す。<br>
    出力次元は入力O'と同じ

<br>

## 2.2.6 残差結合

<figure markdown="span">
  ![Image title](<../../images/2.2 エンコーダ/2.2 エンコーダ_J0133291-13.png>){ width="300" }
  <figcaption>residual connection</figcaption>
</figure>

<div class="chat-container">
  <!-- 春日部つむぎの解説 -->
  <div class="chat left">
    <img src="../../images/2.1 概要/2.1 概要_J0133291.png" alt="春日部つむぎ" class="avatar">
    <div class="chat-message tsumugi-message">
      じゃあ、次は<strong>残差結合（Residual Connection）</strong>について説明するね！💡<br>
      残差結合っていうのは、Attention層とかフィードフォワード層の結果に、<br>
      元の入力を足し戻す仕組みのことだよ。<br>
      これを使うことで、深い層に進むときに情報が失われないようにするんだ✨
    </div>
  </div>

  <!-- ずんだもんの右寄せ質問 -->
  <div class="chat right">
    <div class="chat-message zundamon-message">
      なんで元の入力を足すのだ？<br>
      足さなくても良い気がするのだ？
    </div>
    <img src="../../images/2.1 概要/2.1 概要_J0133291-4.png" alt="ずんだもん" class="avatar">
  </div>

  <!-- 春日部つむぎの続き -->
  <div class="chat left">
    <img src="../../images/2.1 概要/2.1 概要_J0133291-5.png" alt="春日部つむぎ" class="avatar">
    <div class="chat-message tsumugi-message">
      そうだよね～、ちょっと不思議だよね！<br>
      実は、ニューラルネットワークの層が深くなっていくと、<br>
      情報が伝わりにくくなったり、うまく学習できなくなることがあるんだ。<br>
      そこで<strong>残差結合</strong>を使って、元の入力を足すことで、<br>
      情報がスムーズに次の層に伝わるようにしてるんだよ😊
    </div>
  </div>

  <!-- ずんだもんの理解 -->
  <div class="chat right">
    <div class="chat-message zundamon-message">
      なるほどなのだ！深い層でも情報をちゃんと保つためなのだな！<br>
      で、どうやって足してるのだ？もっと具体的に教えてほしいのだ！
    </div>
    <img src="../../images/2.1 概要/2.1 概要_J0133291-6.png" alt="ずんだもん" class="avatar">
  </div>

  <!-- 春日部つむぎの具体的な説明 -->
  <div class="chat left">
    <img src="../../images/2.1 概要/2.1 概要_J0133291.png" alt="春日部つむぎ" class="avatar">
    <div class="chat-message tsumugi-message">
      式で言うとこんな感じ👇<br>
      $$ X^{(k+1)} = \mathcal{F}^{(k)}(X^{(k)}) + X^{(k)} $$
      ここで、<strong>\( X^{(k)} \)</strong> は元の入力で、<strong>\( \mathcal{F}^{(k)}(X^{(k)}) \)</strong> がその層の出力だよ！<br>
      Attentionやフィードフォワード層の結果に、<br>
      元の入力を足して、次の層にちゃんと伝わるようにしてるの！💡
    </div>
  </div>
</div>

!!! abstract
    Residual Connectionは、ニューラルネットワークの層で、出力に元の入力を足し戻すことで、<br>
    深い層でも情報をスムーズに伝え、学習を安定させる仕組

!!! tips
    ResNet（Residual Network）は、2015年に発表された画像認識のためのCNNで<br>
    residual connectionが使用されている

<br>

## 2.2.7 層正規化

<figure markdown="span">
  ![Image title](<../../images/2.2 エンコーダ/2.2 エンコーダ_J0133291-14.png>){ width="300" }
  <figcaption>layer normalization</figcaption>
</figure>

<div class="chat-container">
  <!-- 春日部つむぎの解説 -->
  <div class="chat left">
    <img src="../../images/2.1 概要/2.1 概要_J0133291.png" alt="春日部つむぎ" class="avatar">
    <div class="chat-message tsumugi-message">
      <strong>層正規化（Layer Normalization）</strong>について説明するね！💡<br>
      層正規化は、NNでデータの値が大きすぎたり小さすぎたりするのを防ぐために使われるんだ。<br>
      これで安定して学習できるようになるよ！
    </div>
  </div>

  <!-- ずんだもんの右寄せ質問 -->
  <div class="chat right">
    <div class="chat-message zundamon-message">
      どうやって値を調整してるのだ？<br>
      何か特別な計算とかしてるのか？
    </div>
    <img src="../../images/2.1 概要/2.1 概要_J0133291-4.png" alt="ずんだもん" class="avatar">
  </div>

  <!-- 春日部つむぎの説明の続き -->
  <div class="chat left">
    <img src="../../images/2.1 概要/2.1 概要_J0133291.png" alt="春日部つむぎ" class="avatar">
    <div class="chat-message tsumugi-message">
      そうそう、層正規化ではまず、<br>
      各入力ベクトルの<strong>平均 \( \mu_x \)</strong>と<strong>標準偏差 \( \sigma_x \)</strong>を計算するんだ。<br>
      その後で、各要素 \( x_k \) を正規化して、次の式で求めてるよ👇<br>
      $$ \text{layernorm}(x)_k = g_k \frac{x_k - \mu_x}{\sigma_x + \epsilon} + b_k $$
    </div>
  </div>

  <!-- ずんだもんの理解 -->
  <div class="chat right">
    <div class="chat-message zundamon-message">
      おお～、計算して調整してるのか！<br>
      \( g_k \) と \( b_k \) って何なのだ？
    </div>
    <img src="../../images/2.1 概要/2.1 概要_J0133291-4.png" alt="ずんだもん" class="avatar">
  </div>

  <!-- 春日部つむぎの補足説明 -->
  <div class="chat left">
    <img src="../../images/2.1 概要/2.1 概要_J0133291-3.png" alt="春日部つむぎ" class="avatar">
    <div class="chat-message tsumugi-message">
      \( g_k \) は<strong>ゲイン（gain）</strong>、\( b_k \) は<strong>バイアス</strong>で、<br>
      出力のスケーリングと調整をするためのパラメータだよ！<br>
      これを使うことで、正規化した後の値をさらに微調整してるんだ✨
    </div>
  </div>
</div>

!!! abstract
    layer normalizationをすることで過剰に大きな値になることを防ぎ計算を安定化させる。

<br>

## 2.2.8 ドロップアウト

<div class="chat-container">
  <!-- 春日部つむぎの解説 -->
  <div class="chat left">
    <img src="../../images/2.1 概要/2.1 概要_J0133291.png" alt="春日部つむぎ" class="avatar">
    <div class="chat-message tsumugi-message">
      じゃあ、次は<strong>ドロップアウト（Dropout）</strong>だよ～！💡<br>
      ドロップアウトは、モデルが訓練中に特定の特徴に依存しすぎないように、<br>
      ランダムで一部のニューロンを無効化して、過剰適合（オーバーフィッティング）を防ぐ仕組みなんだ！
    </div>
  </div>

  <!-- ずんだもんの右寄せ質問 -->
  <div class="chat right">
    <div class="chat-message zundamon-message">
      ランダムに無効にしちゃうのか!?<br>
      それでちゃんと学習できるのか心配なのだ！
    </div>
    <img src="../../images/2.2 エンコーダ/2.2 エンコーダ_J0133291-9.png" alt="ずんだもん" class="avatar">
  </div>

  <!-- 春日部つむぎの続き -->
  <div class="chat left">
    <img src="../../images/2.1 概要/2.1 概要_J0133291-5.png" alt="春日部つむぎ" class="avatar">
    <div class="chat-message tsumugi-message">
      大丈夫だよ～😊 訓練中だけで、<br>
      推論時には全部のニューロンが使われるから！<br>
      訓練中は出力ベクトルの要素を20%くらい無効化して、<br>
      推論時には全部使うって感じなんだ✨
    </div>
  </div>

  <!-- ずんだもんの納得 -->
  <div class="chat right">
    <div class="chat-message zundamon-message">
      それなら安心なのだ！<br>
      で、Transformerではどこで使われてるのだ？
    </div>
    <img src="../../images/2.1 概要/2.1 概要_J0133291-4.png" alt="ずんだもん" class="avatar">
  </div>

  <!-- 春日部つむぎの補足説明 -->
  <div class="chat left">
    <img src="../../images/2.1 概要/2.1 概要_J0133291-3.png" alt="春日部つむぎ" class="avatar">
    <div class="chat-message tsumugi-message">
      Transformerでは、<br>
      - input embeddingの後<br>
      - Multi-Head Attention層の出力<br>
      - 残差結合を適用する前の出力<br>
      - Feed Forward層の出力<br>
      で使われてるよ！💡 各層で過剰適合を防ぐためなんだよ～！
    </div>
  </div>
</div>

!!! abstract
    Dropoutをすることでover fittingを防ぐ
