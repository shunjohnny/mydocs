---
title: 2.3 エンコーダ・デコーダ
tags:
  - 大規模言語モデル入門Ⅰ
---

![alt text](<../../images/2.3 エンコーダ・デコーダ/2.3 エンコーダ・デコーダ_J0133291-1.png>)

| **記号**  | **説明**                                 |
|-----------|------------------------------------------|
| **Z**     | Multi-Head Attentionの結果のFeed Forward後      |
| **K', V'**    | encoderのoutputから作成したK,V                     |
| **Q'**    | Masked Multi-Head Attentionの結果をAdd & Normしたもの   |
| **Q, K, V**| Query, Key, Value                       |
| **X**     | Eに位置符号を追加したもの                 |
| **E**     | Embedding後のtoken                       |
| **D**     | 入力Embeddingの次元数D（入力単語をD次元のベクトルに変換）|
| **N**     | 語彙数（token数）                         |

<div class="chat-container">
  <!-- 春日部つむぎの解説 -->
  <div class="chat left">
    <img src="../../images/2.2 エンコーダ/2.2 エンコーダ_J0133291-1.png" alt="春日部つむぎ" class="avatar">
    <div class="chat-message tsumugi-message">
      じゃあ次は<strong>Decoder</strong>の説明だね！💡<br>
      Encoderと違うところがいくつかあるんだよ。<br>
      まずは、Decoderでは<strong>Masked Multi-Head Attention</strong>を使って、<br>
      まだ生成してない未来の単語を見ないように<br>
      マスクをかけて計算してるんだ！
    </div>
  </div>

  <!-- ずんだもんの右寄せ質問 -->
  <div class="chat right">
    <div class="chat-message zundamon-message">
      なるほどなのだ！未来の単語を隠すのだな！<br>
      ほかには何が違うのだ？
    </div>
    <img src="../../images/2.1 概要/2.1 概要_J0133291-6.png" alt="ずんだもん" class="avatar">
  </div>

  <!-- 春日部つむぎの続き -->
  <div class="chat left">
    <img src="../../images/2.1 概要/2.1 概要_J0133291-5.png" alt="春日部つむぎ" class="avatar">
    <div class="chat-message tsumugi-message">
      そうだね、あと<strong>Cross Attention</strong>も大事だよ！<br>
      DecoderはEncoderが作った情報を使って次に出す単語を予測してるの。<br>
      これで文脈をしっかり捉えるんだ✨<br>
      そして最後に、出力はsoftmaxで計算して、次に出す単語を決めるよ～😊
    </div>
  </div>
</div>

<br>

## 2.3.1 交差注意機構
---

<figure markdown="span">
  ![Image title](<../../images/2.3 エンコーダ・デコーダ/2.3 エンコーダ・デコーダ_J0133291-2.png>){ width="600" }
  <figcaption>cross attention</figcaption>
</figure>

<div class="chat-container">
  <!-- 春日部つむぎの解説 -->
  <div class="chat left">
    <img src="../../images/2.1 概要/2.1 概要_J0133291.png" alt="春日部つむぎ" class="avatar">
    <div class="chat-message tsumugi-message">
      じゃあ<strong>Cross Attention</strong>について説明するね！💡<br>
      これは、EncoderとDecoderの間で情報をやり取りするための <br>
      大事な仕組みなんだ～！<br>
      Decoderが文を生成する時に、<br>
      Encoderが処理した入力文の情報を参照しながら、<br>
      次の単語を決めていくんだ✨
    </div>
  </div>

  <!-- 春日部つむぎの続き -->
  <div class="chat left">
    <img src="../../images/2.1 概要/2.1 概要_J0133291-3.png" alt="春日部つむぎ" class="avatar">
    <div class="chat-message tsumugi-message">
      Cross Attentionでは、<br>
      Decoderから<strong>Query（Q）</strong>を、<br>
      Encoderから<strong>Key（K）</strong>と<strong>Value（V）</strong>を使うんだ💡<br>
      簡単に言うと、<br>
      Decoderが今どこを注目してるか（Query）を、<br>
      Encoderが作った文脈情報（Key, Value）と合わせて、<br>
      次に生成すべき単語を決めるの！
    </div>
  </div>

  <!-- ずんだもんの理解 -->
  <div class="chat right">
    <div class="chat-message zundamon-message">
      DecoderがQueryを出して、<br>
      それにEncoderの情報を使う感じなのだな！<br>
      で、具体的にはどうやって計算するのだ？
    </div>
    <img src="../../images/2.1 概要/2.1 概要_J0133291-4.png" alt="ずんだもん" class="avatar">
  </div>

  <!-- 春日部つむぎの式の説明 -->
  <div class="chat left">
    <img src="../../images/2.1 概要/2.1 概要_J0133291.png" alt="春日部つむぎ" class="avatar">
    <div class="chat-message tsumugi-message">
      式で説明するとこうなるよ👇<br>
      $$ \text{Attention}(Q, K, V) = \text{softmax}\left(\frac{QK^T}{\sqrt{d_k}}\right)V $$
      さっきのMulti-Head Attintionと計算式自体は同じだね😊
    </div>
  </div>
</div>

!!! abstract
    Cross AttentionではQ:Decoderからの出力、K,V:Encoderからの出力を使用

<br>

## 2.3.2 トークン出力分布の計算
---
<div class="chat-container">
  <!-- 春日部つむぎの解説 -->
  <div class="chat left">
    <img src="../../images/2.1 概要/2.1 概要_J0133291-3.png" alt="春日部つむぎ" class="avatar">
    <div class="chat-message tsumugi-message">
      次は、<strong>トークン出力分布の計算</strong>について説明するね！💡<br>
      Decoderは、次に出す単語（トークン）の確率分布を計算して、どの単語を生成するかを決めるんだ。<br>
      例えば、この式👇
      $$ P(N_{i+1} | n_1, n_2, ..., n_M, N_1, N_2, ..., N_i) = \text{softmax}(Z'W) $$
      ここで、<strong>W</strong>はモデルが定義する語彙数に対応する重み行列で、<br>
      Decoderが生成したベクトル \( Z' \) を変換して、<br>
      次の単語 \( N_{i+1} \) を予測するために使われるんだよ～！<br>
      この線形変換を使って、モデルの次元数から語彙数に変換してるんだ。
    </div>
  </div>

  <!-- ずんだもんの質問 -->
  <div class="chat right">
    <div class="chat-message zundamon-message">
      なるほどなのだ！<br>
      でも、最初の単語ってどうやって予測するのだ？<br>
    </div>
    <img src="../../images/2.1 概要/2.1 概要_J0133291-8.png" alt="ずんだもん" class="avatar">
  </div>

  <!-- 春日部つむぎの続き -->
  <div class="chat left">
    <img src="../../images/2.2 エンコーダ/2.2 エンコーダ_J0133291-16.png" alt="春日部つむぎ" class="avatar">
    <div class="chat-message tsumugi-message">
      最初の単語を予測するときは、実際の入力がないから、特別な「開始トークン」を使ってスタートするんだ～😊<br>
      それを \( N_0 \) として、Decoderに渡して、次の単語を予測していくんだよ！<br>
      その後も、Decoderは出力するたびに前の単語を使って次の単語を予測していくんだ！
    </div>
  </div>
</div>

!!! abstract
    Decoderの出力をモデルが定義する語彙数を使用して線形変換し、softmaxをすることで、<br>
    次に来る語彙を予測する確率分布を出力する。

<br>

## 2.3.3 注意機構のマスク処理
---
<div class="chat-container">
  <!-- 春日部つむぎの解説 -->
  <div class="chat left">
    <img src="../../images/2.1 概要/2.1 概要_J0133291.png" alt="春日部つむぎ" class="avatar">
    <div class="chat-message tsumugi-message">
      次はDecoderで使う<strong>Masked Multi-Head Attention</strong>の仕組みについて<br>
      詳しく説明するね！💡<br>
      Decoderでは、次に生成する単語を予測するとき、<br>
      まだ生成していない「未来の単語」を見ちゃうと不正確な予測になっちゃうから、<br>
      それを避けるために、<strong>マスク処理</strong>をしているんだ。<br>
      Attentionの計算中に、未来の単語に注目しないようにするため、<br>
      特定の部分を無効化してるんだよ～。
    </div>
  </div>

  <!-- ずんだもんの質問 -->
  <div class="chat right">
    <div class="chat-message zundamon-message">
      マスクを掛けるってどういうことなのだ？<br>
      無効化って、どうやって計算してるのか教えてほしいのだ！
    </div>
    <img src="../../images/2.1 概要/2.1 概要_J0133291-2.png" alt="ずんだもん" class="avatar">
  </div>

  <!-- 春日部つむぎの追加説明 -->
  <div class="chat left">
    <img src="../../images/2.1 概要/2.1 概要_J0133291-3.png" alt="春日部つむぎ" class="avatar">
    <div class="chat-message tsumugi-message">
      いいよ～！💡<br>
      マスク処理っていうのは、<br>
      <strong>Attention</strong>の計算の途中で<br>
      未来の単語に対する重みをゼロに近づけるようにすることなんだ。<br>
      具体的には、次のような式で計算してるんだよ👇<br>
      $$ \text{Attention}(Q, K, V) = \text{softmax}\left(\frac{QK^T}{\sqrt{d_k}} + M \right)V $$
      もう見慣れた式だよね😊<br>
      ここで、<strong>M</strong>がマスクなんだ！<br>
      マスク部分には、まだ生成していない未来の単語に対して大きな負の値を入れてるんだよ。<br>
      これで、未来の単語に対する重みがsoftmaxの結果でゼロに近づいて、<br>
      無視されるってわけ！
    </div>
  </div>

  <!-- ずんだもんの理解 -->
  <div class="chat right">
    <div class="chat-message zundamon-message">
      大きな負の値を入れて、<br>
      マスクで無効化しているのだな！<br>
      それで未来の情報を見ないようにしてるんだな！<br>
      完全にtransformerを理解したのだ！
    </div>
    <img src="../../images/2.1 概要/2.1 概要_J0133291-4.png" alt="ずんだもん" class="avatar">
  </div>
</div>

!!! abstract
    未来の単語を予測に使用しないように、未来の単語の重みをゼロに近づけるマスク処理を行う

<br>

<div class="chat-container">
  <!-- 春日部つむぎの解説 -->
  <div class="chat left">
    <img src="../../images/2.1 概要/2.1 概要_J0133291-5.png" alt="春日部つむぎ" class="avatar">
    <div class="chat-message tsumugi-message">
      あーしのtransformerの説明は終了だよ!<br>
      2.1の概要で説明したこととか<br>
      2.2, 2.3の初めの図の意味が今なら分かるはずだから、<br>
      もう一度見直して、transformerの処理を確認してみてね!
    </div>
  </div>
</div>